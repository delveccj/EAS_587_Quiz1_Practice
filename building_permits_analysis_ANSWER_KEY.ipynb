{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# EAS 587 Quiz 1 Practice - Building Permits Analysis\n",
    "\n",
    "This notebook provides practice for Quiz 1 focusing on data analysis and K-means clustering techniques.\n",
    "\n",
    "**Time Target: 20 minutes**\n",
    "\n",
    "## Dataset\n",
    "Buffalo building permits data from 2015 to present, containing information about:\n",
    "- Permit types (electrical, plumbing, heating, construction, etc.)\n",
    "- Geographic locations\n",
    "- Fees and work values\n",
    "- Contractor information\n",
    "- Neighborhoods and districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/delveccj/EAS_587_Quiz1_Practice/main/data/permits_2015_present.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xeu95ffyd7",
   "source": "## Quiz Questions\n\nComplete the following questions by writing code in the cells below each question.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "0s6vljp2dz1r",
   "source": "ðŸ” **Question 1:** Show the shape of this dataframe",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ybbpujkvye",
   "source": "# Answer:\ndf.shape",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nk37ysbbxn",
   "source": "ðŸ” **Question 2:** Display the first 10 rows",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "y561tpiuf9o",
   "source": "# Answer\ndf.head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4vposa7u6yx",
   "source": "ðŸ” **Question 3:** Check for missing values in the dataset. Display the count of missing values and percentage missing for each column that has missing data. Format the output as a DataFrame with columns 'Missing Count' and 'Missing Percentage'.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h3pt81zuti5",
   "source": "# Answer\nmissing_values = df.isnull().sum()\nmissing_percentage = (df.isnull().sum() / len(df)) * 100\n\nmissing_summary = pd.DataFrame({\n    'Missing Count': missing_values,\n    'Missing Percentage': missing_percentage\n})\n\n# Show only columns with missing values\nmissing_summary[missing_summary['Missing Count'] > 0]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "suqhdhjj6qf",
   "source": "ðŸ” **Question 4:** Drop columns that are missing more than 90% of their data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8k0ncg7854n",
   "source": "# Answer\n# Calculate percentage missing for each column\nmissing_percentage = (df.isnull().sum() / len(df)) * 100\n\n# Find columns with more than 90% missing data\ncolumns_to_drop = missing_percentage[missing_percentage > 90].index.tolist()\nprint(\"Columns to drop (>90% missing):\", columns_to_drop)\n\n# Drop the columns\ndf_cleaned = df.drop(columns=columns_to_drop)\nprint(f\"Original shape: {df.shape}\")\nprint(f\"After dropping columns: {df_cleaned.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gu0solhaoxh",
   "source": "ðŸ” **Question 5:** Remove rows where both Latitude and Longitude are missing (since we'll need coordinates for clustering)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "aeeou92gsq8",
   "source": "# Answer\n# Remove rows where both Latitude and Longitude are missing\ndf_cleaned = df_cleaned.dropna(subset=['Latitude', 'Longitude'])\n\nprint(f\"After removing rows with missing coordinates: {df_cleaned.shape}\")\nprint(f\"Rows removed: {df.shape[0] - df_cleaned.shape[0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dex07l7n3o",
   "source": "ðŸ” **Question 6:** Look at the Fees column. What data cleaning operation do you think needs to be performed? Apply that operation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "k908y4m0d9o",
   "source": "# Answer\n# Clean Fees column - remove $ and convert to numeric\ndf_cleaned['Fees'] = df_cleaned['Fees'].str.replace('$', '').str.replace(',', '')\ndf_cleaned['Fees'] = pd.to_numeric(df_cleaned['Fees'], errors='coerce')\n\nprint(\"Fees column cleaned:\")\nprint(df_cleaned['Fees'].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8q8ucpk7y9v",
   "source": "ðŸ” **Question 7:** Based on Question 6 - what other column would you need to clean as well? Go ahead and clean it now.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0lrbbddlpxf",
   "source": "# Answer\n# Clean Value of Work column - same issue as Fees (has $ and commas)\ndf_cleaned['Value of Work'] = df_cleaned['Value of Work'].str.replace('$', '').str.replace(',', '')\ndf_cleaned['Value of Work'] = pd.to_numeric(df_cleaned['Value of Work'], errors='coerce')\n\nprint(\"Value of Work column cleaned:\")\nprint(df_cleaned['Value of Work'].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d8wu8qn34gr",
   "source": "ðŸ” **Question 8:** Create tabular histogram counts that show the number of permits in ranges: '$0-1K', '$1K-5K', '$5K-25K', '$25K-100K', '$100K-500K', '$500K+'",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kz9wrts5c7",
   "source": "# Answer\n# Show distribution in value ranges (more informative than histogram)\nvow = df_cleaned['Value of Work'].dropna()\n\n# Create value range bins\nbins = [0, 1000, 5000, 25000, 100000, 500000, np.inf]\nlabels = ['$0-1K', '$1K-5K', '$5K-25K', '$25K-100K', '$100K-500K', '$500K+']\n\n# Count permits in each range\nvalue_ranges = pd.cut(vow, bins=bins, labels=labels, include_lowest=True)\nrange_counts = value_ranges.value_counts().sort_index()\n\nprint(\"Value of Work Distribution:\")\nprint(range_counts)\nprint(f\"\\nTop 10 highest value projects:\")\nprint(df_cleaned.nlargest(10, 'Value of Work')[['Address', 'Permit Type', 'Value of Work', 'Description of Work']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ug8pmschhf",
   "source": "ðŸ” **Question 9:** Create a new dataframe called `df_kmeans` for clustering analysis. Remove permits with Value of Work over $500,000 and keep only rows where Value of Work and Fees are not null.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kqgucpoy8yd",
   "source": "# Answer\n# Create dataframe for K-means clustering\ndf_kmeans = df_cleaned.copy()\n\n# Remove high-value outliers (>$500K)\ndf_kmeans = df_kmeans[df_kmeans['Value of Work'] <= 500000]\n\n# Keep only rows with complete data for clustering features\ndf_kmeans = df_kmeans.dropna(subset=['Value of Work', 'Fees'])\n\nprint(f\"Original cleaned data: {df_cleaned.shape}\")\nprint(f\"K-means ready data: {df_kmeans.shape}\")\nprint(f\"Removed {df_cleaned.shape[0] - df_kmeans.shape[0]} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0klpdkwo8trn",
   "source": "ðŸ” **Question 10:** Perform an elbow analysis to determine the optimal number of clusters for contractor and Value of Work clustering. Plot the Within-Cluster Sum of Squares (WCSS) for k values from 1 to 10 using Value of Work and Contractor License Number as features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xoy6y9j70y",
   "source": "# Answer\n# Prepare data for elbow analysis\ndf_contractor = df_kmeans.dropna(subset=['Contractor License Number']).copy()\n\n# Encode contractor license numbers\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_contractor['Contractor_Encoded'] = le.fit_transform(df_contractor['Contractor License Number'])\n\n# Prepare features for clustering\nfeatures = ['Value of Work', 'Contractor_Encoded']\nX = df_contractor[features].copy()\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Perform elbow analysis\nwcss = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\n\n# Plot the elbow curve\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, wcss, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.title('Elbow Method for Optimal Number of Clusters')\nplt.grid(True)\nplt.show()\n\nprint(\"WCSS values for each k:\")\nfor k, w in zip(k_range, wcss):\n    print(f\"k={k}: WCSS={w:,.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yl7om807ym",
   "source": "ðŸ” **Question 11:** Based on your elbow analysis, perform K-means clustering using Value of Work and Contractor License Number. Use the optimal number of clusters from Question 10 and provide a detailed analysis showing the characteristics of each cluster (average values, top contractors, license types).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kl0l5edisg",
   "source": "# Answer\n# Prepare data for clustering with contractors\ndf_contractor = df_kmeans.dropna(subset=['Contractor License Number']).copy()\n\n# Encode contractor license numbers (categorical to numeric)\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_contractor['Contractor_Encoded'] = le.fit_transform(df_contractor['Contractor License Number'])\n\n# Prepare features for clustering\nfeatures = ['Value of Work', 'Contractor_Encoded']\nX = df_contractor[features].copy()\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply K-means clustering\nkmeans = KMeans(n_clusters=5, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to dataframe\ndf_contractor['Cluster'] = clusters\n\nprint(f\"Clustering complete with {len(df_contractor)} permits\")\nprint(f\"Number of unique contractors: {df_contractor['Contractor License Number'].nunique()}\")\nprint(f\"Cluster distribution:\\n{pd.Series(clusters).value_counts().sort_index()}\")\n\nprint(\"CLUSTER ANALYSIS BY VALUE OF WORK AND CONTRACTOR PATTERNS:\\n\")\n\nfor cluster_id in range(5):\n    cluster_data = df_contractor[df_contractor['Cluster'] == cluster_id]\n    \n    print(f\"CLUSTER {cluster_id} ({len(cluster_data)} permits):\")\n    print(f\"  Average Value of Work: ${cluster_data['Value of Work'].mean():,.2f}\")\n    print(f\"  Median Value of Work: ${cluster_data['Value of Work'].median():,.2f}\")\n    print(f\"  Most common License Type: {cluster_data['License Type'].mode().iloc[0] if not cluster_data['License Type'].mode().empty else 'N/A'}\")\n    \n    # Top contractors in this cluster by permit count\n    top_contractors = cluster_data.groupby(['Contractor License Number', 'Applicant']).size().reset_index(name='permit_count')\n    top_contractors = top_contractors.sort_values('permit_count', ascending=False).head(3)\n    \n    print(f\"  Top 3 contractors by permit count:\")\n    for i, (idx, row) in enumerate(top_contractors.iterrows(), 1):\n        print(f\"    {i}. {row['Applicant']} (License: {row['Contractor License Number']}) - {row['permit_count']} permits\")\n    \n    # Value of Work distribution\n    print(f\"  Value range: ${cluster_data['Value of Work'].min():,.2f} - ${cluster_data['Value of Work'].max():,.2f}\")\n    print(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}